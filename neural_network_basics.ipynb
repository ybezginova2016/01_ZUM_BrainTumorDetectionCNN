{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c1a8c8",
   "metadata": {},
   "source": [
    "# Training basic DL neural network\n",
    "\n",
    "How exactly will this learning take place, given the Layer and Operation classes just defined? Recalling how the model from the last chapter worked, we’ll implement the\n",
    "following:\n",
    "- 1. The neural network should take X and pass it successively forward through each\n",
    "Layer (which is really a convenient wrapper around feeding it through many\n",
    "Operations), at which point the result will represent the prediction.\n",
    "- 2. Next, prediction should be compared with the value y to calculate the loss and\n",
    "generate the “loss gradient,” which is the partial derivative of the loss with respect\n",
    "to each element in the last layer in the network (namely, the one that generated\n",
    "the prediction).\n",
    "- 3. Finally, we’ll send this loss gradient successively backward through each layer,\n",
    "along the way computing the “parameter gradients”—the partial derivative of the\n",
    "loss with respect to each of the parameters—and storing them in the corresponding\n",
    "Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setting layers\n",
    "First, we’ll want our neural network to ultimately deal with Layers the same way our Layers dealt with Operations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The forward method to receive X as input:\n",
    "for layer in self.layers:\n",
    "    X = layer.forward(X)\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The backward method to receive X as input:\n",
    "for layer in reversed(self.layers):\n",
    "    grad = layer.backward(grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Where will grad come from? It has to come from the loss, a special function that takes in the prediction along with y and:\n",
    "\n",
    "• Computes a single number representing the “penalty” for the network making that prediction.\n",
    "\n",
    "• Sends backward a gradient for every element of the prediction with respect to the loss. This gradient is what the last Layer in the network will receive as the input to its backward function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    The class for a neural network.\n",
    "    '''\n",
    "    def __init__(self, layers: List[Layer],\n",
    "    loss: Loss,\n",
    "    seed: float = 1)\n",
    "    '''\n",
    "    Neural networks need layers, and a loss.\n",
    "    '''\n",
    "    self.layers = layers\n",
    "    self.loss = loss\n",
    "    self.seed = seed\n",
    "    if seed:\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'seed', self.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our First Deep Learning Model (from Scratch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6945d65d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m deep_neural_network \u001B[38;5;241m=\u001B[39m \u001B[43mNeuralNetwork\u001B[49m(\n\u001B[1;32m      2\u001B[0m     layers\u001B[38;5;241m=\u001B[39m[Dense(neurons\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m13\u001B[39m,\n\u001B[1;32m      3\u001B[0m                 activation\u001B[38;5;241m=\u001B[39mSigmoid()),\n\u001B[1;32m      4\u001B[0m             Dense(neurons\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m13\u001B[39m,\n\u001B[1;32m      5\u001B[0m                 activation\u001B[38;5;241m=\u001B[39mSigmoid()),\n\u001B[1;32m      6\u001B[0m             Dense(neurons\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m      7\u001B[0m                 activation\u001B[38;5;241m=\u001B[39mLinearAct())],\n\u001B[1;32m      8\u001B[0m     loss\u001B[38;5;241m=\u001B[39mMeanSquaredError(),\n\u001B[1;32m      9\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m\n\u001B[1;32m     10\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "deep_neural_network = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                activation=LinearAct())],\n",
    "    loss=MeanSquaredError(),\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49937a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998a3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
